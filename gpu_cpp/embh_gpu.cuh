#pragma once

#include <cuda_runtime.h>
#include <cstdint>
#include <vector>
#include <string>

// GPU data structures for pattern grouping and message passing

struct GPUPatternGroups {
    // Edge topology (host side, stored for reference)
    int num_edges;
    int num_patterns;
    int num_taxa;

    // Per-edge group information
    int* d_edge_num_groups;           // [num_edges] number of groups per edge
    int* d_edge_group_offsets;        // [num_edges+1] cumulative offsets into group arrays
    int* d_edge_subtree_sizes;        // [num_edges] subtree size for each edge

    // Group signatures (packed)
    // For edge e, group g: signature starts at d_group_signatures[d_edge_group_offsets[e] * max_subtree_size + g * subtree_size[e]]
    uint8_t* d_group_signatures;      // [total_groups * max_subtree_size] packed signatures
    int max_subtree_size;
    int total_groups;

    // Pattern to group mapping
    int* d_pattern_to_group;          // [num_patterns * num_edges] for pattern p, edge e: group_id

    // Group weights (sum of pattern weights in group)
    int* d_group_weights;             // [total_groups]

    // Pattern weights
    int* d_pattern_weights;           // [num_patterns]

    // Pattern bases
    uint8_t* d_pattern_bases;         // [num_patterns * num_taxa] base at each taxon

    // Tree structure
    int* d_parent_edge;               // [num_edges] parent edge index (-1 for root children)
    int* d_child_edges;               // [num_edges * 2] up to 2 children per edge (-1 if none)
    int* d_edge_depth;                // [num_edges] depth in tree (leaves=0, root children have max depth)
    int* d_post_order;                // [num_edges] edges in post-order (leaves first)

    // Leaf mapping
    int* d_edge_leaf_indices;         // [num_edges * max_subtree_size] leaf indices for each edge's subtree
};

struct GPUTransitionMatrices {
    // Transition matrices P(child|parent) for each edge
    double* d_trans_matrices;         // [num_edges * 16] row-major 4x4 matrices
    double* d_root_prob;              // [4] root probability distribution
};

struct GPUMessageBuffers {
    // Computed messages for each group
    double* d_group_messages;         // [total_groups * 4] message vectors
    double* d_group_log_scales;       // [total_groups] log scaling factors

    // Per-pattern results (after broadcasting)
    double* d_pattern_beliefs;        // [num_patterns * num_edges * 16] beliefs at each edge
    double* d_pattern_log_scales;     // [num_patterns] accumulated log scaling factors
    double* d_pattern_log_likelihoods; // [num_patterns] final log-likelihood per pattern
};

// Host-side helper to load precomputed groups from files
class GPUPatternGroupsLoader {
public:
    // Load from .group_* files generated by pattern_groups_precompute
    static GPUPatternGroups load_from_files(
        const std::string& prefix,
        const std::string& tree_edges_file,
        const std::string& pattern_file
    );

    // Free GPU memory
    static void free_gpu_memory(GPUPatternGroups& groups);
    static void free_gpu_memory(GPUTransitionMatrices& matrices);
    static void free_gpu_memory(GPUMessageBuffers& buffers);

    // Allocate message buffers
    static GPUMessageBuffers allocate_message_buffers(const GPUPatternGroups& groups);

    // Load transition matrices from tree file
    static GPUTransitionMatrices load_transition_matrices(
        const std::string& tree_edges_file,
        const double* root_prob  // [4] array
    );
};

// CUDA kernels for log-likelihood computation

// Kernel 1: Compute unique upward messages for each signature group
// Parallelizes over (edge, group) pairs - only 7086 threads instead of 47304!
__global__ void compute_upward_messages_for_groups(
    int num_edges,
    const int* edge_num_groups,
    const int* edge_group_offsets,
    const int* edge_subtree_sizes,
    const uint8_t* group_signatures,
    int max_subtree_size,
    const int* edge_leaf_indices,
    const double* trans_matrices,
    const int* post_order,
    const int* child_edges,
    double* group_messages,
    double* group_log_scales
);

// Kernel 2: Apply computed messages to all patterns (broadcast)
// Parallelizes over patterns
__global__ void broadcast_messages_to_patterns(
    int num_patterns,
    int num_edges,
    const int* pattern_to_group,
    const int* edge_group_offsets,
    const double* group_messages,
    const double* group_log_scales,
    double* pattern_log_scales
);

// Kernel 3: Compute final log-likelihood for each pattern
__global__ void compute_pattern_log_likelihoods(
    int num_patterns,
    const double* pattern_log_scales,
    const double* root_beliefs,  // [num_patterns * 4] marginal at root
    const double* root_prob,      // [4]
    double* pattern_log_likelihoods
);

// Kernel 4: Weighted sum of log-likelihoods (reduction)
__global__ void weighted_sum_reduction(
    int num_patterns,
    const double* pattern_log_likelihoods,
    const int* pattern_weights,
    double* partial_sums,
    int num_blocks
);

// Host function to compute log-likelihood using GPU
double compute_log_likelihood_gpu(
    const GPUPatternGroups& groups,
    const GPUTransitionMatrices& matrices,
    GPUMessageBuffers& buffers
);

#endif // EMBH_GPU_CUH
