CUDA PARALLELIZATION OPPORTUNITIES FOR EMBH PHYLOGENETIC INFERENCE
====================================================================

Generated: November 2025
Codebase: /home/pk/projects/embh/cuda_cpp/

================================================================================
HIGHEST PRIORITY - PATTERN-LEVEL PARALLELIZATION
================================================================================

1. ComputeLogLikelihoodUsingPatterns()
   File: embh_core.cpp
   Lines: 2577-2740 (main loop at 2606-2700)
   Description: Log-likelihood computation over 648 patterns
   Parallelism: Each pattern is COMPLETELY INDEPENDENT
   Expected Benefit: HIGH (linear speedup with GPU threads)
   Strategy: Launch 648 CUDA threads, one per pattern
   Reduction: Sum pattern likelihoods with parallel reduction

2. ComputeLogLikelihoodUsingPatternsWithPropagation()
   File: embh_core.cpp
   Lines: 2746-2821 (main loop at 2795-2815)
   Description: Pattern-wise belief propagation likelihood
   Parallelism: 648 independent patterns
   Expected Benefit: HIGH
   Operations per pattern:
   - SetSite() - set pattern index
   - InitializePotentialAndBeliefs() - independent
   - CalibrateTree() - message passing (partially parallelizable)
   - Marginalization (4-element reduction)
   - Likelihood accumulation (atomic add)

3. ComputeLogLikelihoodUsingPatternsWithPropagationOptimized()
   File: embh_core.cpp
   Lines: 2987-3065 (main loop at 3041-3065)
   Description: Optimized version with memoization
   Parallelism: Fully parallelizable across 648 patterns
   Expected Benefit: HIGH

4. ComputeLogLikelihoodUsingPatternsWithPropagationMemoized()
   File: embh_core.cpp
   Lines: 3067-3180 (main loop at 3131-3155)
   Description: Memoized message caching version
   Parallelism: Per-pattern with thread-local caches
   Expected Benefit: HIGH
   Consideration: Message caches must be per-pattern to avoid race conditions

5. ComputeLogLikelihood() - Non-Pattern Version
   File: embh_core.cpp
   Lines: 2479-2575 (main loop at 2487-2574)
   Description: Site-by-site likelihood computation
   Parallelism: Each site independent
   Expected Benefit: HIGH
   Data per thread:
   - Site index
   - Shared tree structure (read-only)
   - Shared transition matrices (read-only)
   - Thread-local conditionalLikelihoodMap

================================================================================
HIGH PRIORITY - EM ALGORITHM E-STEP
================================================================================

6. ComputeExpectedCounts() - Main E-Step Loop
   File: embh_core.cpp
   Lines: 2128-2164 (pattern loop over num_dna_patterns)
   Description: Accumulate expected counts across all patterns
   Parallelism: 648 patterns fully independent
   Expected Benefit: HIGH
   Operations per pattern:
   - ApplyEvidenceAndReset() - set site evidence
   - CalibrateTree() - belief propagation
   - SetMarginalProbabilitesForEachEdgeAsBelief() - extract marginals
   - AddToExpectedCountsForEachVariable() - ATOMIC accumulation
   - AddToExpectedCountsForEachEdge() - ATOMIC accumulation
   Synchronization: Atomic operations for count accumulation

7. AddToExpectedCountsForEachVariable()
   File: embh_core.cpp
   Lines: 2085-2105
   Description: Accumulate vertex marginal probabilities
   Parallelism: Each clique marginalizes independently
   Expected Benefit: MEDIUM
   Critical: Requires atomic add to expectedCountsForVertex[v][i]
   Size: 4-element array per vertex

8. AddToExpectedCountsForEachEdge()
   File: embh_core.cpp
   Lines: 2107-2126
   Description: Accumulate edge pair joint probabilities
   Parallelism: Each edge processes independently within pattern
   Expected Benefit: MEDIUM
   Critical: Atomic add for 4x4 matrix per edge pair
   Size: 16 elements per edge

================================================================================
MEDIUM PRIORITY - CLIQUE TREE BELIEF COMPUTATION
================================================================================

9. ComputeBelief() - Independent Belief Computation
   File: embh_core.cpp
   Lines: 373-431
   Description: Compute final belief from messages (after calibration)
   Parallelism: ALL cliques can compute beliefs in parallel
   Expected Benefit: HIGH (within calibration step)
   Operations:
   - Multiply initial potential by messages from neighbors
   - Row-wise or column-wise 4x4 matrix multiplications
   - Normalize/scale factors
   Data: 16 elements (4x4 matrix) per clique

10. MarginalizeOverVariable()
    File: embh_core.cpp
    Lines: 351-370
    Description: Sum over one variable in joint distribution
    Parallelism: Each clique independent
    Expected Benefit: MEDIUM
    Operations: Sum rows or columns of 4x4 matrix → 4-element result

11. CalibrateTree() - Tree Calibration
    File: embh_core.cpp
    Lines: 1265-1289
    Description: Two-pass message passing + belief computation
    Structure:
    - UPWARD PASS (lines 1270-1273): Sequential (leaves→root)
    - DOWNWARD PASS (lines 1278-1281): Sequential (root→leaves)
    - BELIEF COMPUTATION (lines 1284-1286): FULLY PARALLELIZABLE
    Expected Benefit: MEDIUM (only belief step is parallelizable)
    Parallelization Option: Level-synchronous approach for passes

================================================================================
MEDIUM PRIORITY - TREE MESSAGE PASSING
================================================================================

12. SendMessage() - Core Message Passing
    File: embh_core.cpp
    Lines: 1077-1263
    Description: Send message from one clique to neighbor

    A. Product Step (Lines 1114-1180):
       - Multiply messages from neighbors
       - 4x4 matrix element-wise operations
       - Row-wise or column-wise depending on shared variable

    B. Marginalization Step (Lines 1193-1221):
       - Sum over common variable
       - 4x4 → 4-element reduction

    Challenge: Sequential dependency along tree paths
    Parallelization Option: Level-synchronous (process tree levels in parallel)
    Expected Benefit: MEDIUM (limited by tree structure)

================================================================================
MEDIUM PRIORITY - MATRIX OPERATIONS (FINE-GRAINED)
================================================================================

13. 4x4 Row-wise Matrix-Vector Multiplication
    Locations:
    - clique::ComputeBelief() line 391
    - cliqueTree::SendMessage() lines 1135-1136
    Pattern:
    ```cpp
    for (int dna_x = 0; dna_x < 4; dna_x++)
        for (int dna_y = 0; dna_y < 4; dna_y++)
            factor[dna_x][dna_y] *= messageFromNeighbor[dna_y];
    ```
    Parallelism: 16-thread block
    Expected Benefit: MEDIUM (small matrices, but frequent)

14. 4x4 Column-wise Matrix-Vector Multiplication
    Locations:
    - clique::ComputeBelief() line 400
    - cliqueTree::SendMessage() lines 1142-1144
    Pattern:
    ```cpp
    for (int dna_y = 0; dna_y < 4; dna_y++)
        for (int dna_x = 0; dna_x < 4; dna_x++)
            factor[dna_x][dna_y] *= messageFromNeighbor[dna_x];
    ```
    Parallelism: 16-thread block
    Expected Benefit: MEDIUM

15. Matrix Scaling/Normalization
    Locations:
    - clique::ComputeBelief() lines 394-396, 415-417
    - cliqueTree::SendMessage() lines 1147-1150
    Pattern:
    ```cpp
    for (int dna_x = 0; dna_x < 4; dna_x++)
        for (int dna_y = 0; dna_y < 4; dna_y++)
            this->factor[dna_x][dna_y] /= this->scalingFactor;
    ```
    Parallelism: 16-thread block
    Expected Benefit: LOW-MEDIUM

16. Conditional Likelihood Computation
    File: embh_core.cpp
    Lines: 2531-2544
    Pattern:
    ```cpp
    for (int dna_p = 0; dna_p < 4; ++dna_p) {
        double partialLikelihood = 0.0;
        for (int dna_c = 0; dna_c < 4; ++dna_c) {
            partialLikelihood += P[dna_p][dna_c] * childCL[dna_c];
        }
        conditionalLikelihoodMap[p][dna_p] *= partialLikelihood;
    }
    ```
    Parallelism: 4 threads per parent state
    Expected Benefit: MEDIUM (executed 648 × tree_size times)

================================================================================
LOW PRIORITY - PARAMETER UPDATES (M-STEP)
================================================================================

17. ComputeMarginalProbabilitiesUsingExpectedCounts()
    File: embh_core.cpp
    Lines: 2166-2225
    Description: Normalize expected counts to probabilities
    A. Vertex normalization (lines 2172-2193):
       - 4-element array normalization per vertex
       - Independent per vertex
    B. Edge normalization (lines 2196-2224):
       - 4x4 matrix row normalization per edge
       - Independent per edge
    Parallelism: Per-vertex/per-edge (10-100 items)
    Expected Benefit: LOW (small data, fast on CPU)

18. ComputeMLEstimateOfBHGivenExpectedDataCompletion()
    File: embh_core.cpp
    Lines: 3419+
    Description: Update branch length parameters
    Parallelism: Per-edge independent
    Expected Benefit: LOW (small number of edges)

================================================================================
DATA STRUCTURES FOR GPU MEMORY
================================================================================

Clique Tree Structure (constant during EM):
- cliques: vector of ~10-50 cliques
- Per clique:
  - initialPotential: 4x4 matrix (128 bytes)
  - factor: 4x4 matrix (128 bytes)
  - belief: 4x4 matrix (128 bytes)
  - messagesFromNeighbors: ~128 bytes per neighbor

Vertex Data:
- transitionMatrix: 4x4 per vertex (128 bytes each)
- DNAcompressed: pattern bases (8 bytes × 648 patterns = 5.2 KB)

Pattern Data (PackedPatternStorage):
- 3-bit packed representation
- 648 patterns × 38 taxa ≈ 304 bytes (highly compact)

Expected Counts:
- Per vertex: 4 doubles (32 bytes)
- Per edge: 16 doubles (128 bytes)

Total GPU Memory: ~2-5 MB (easily fits in GPU memory)

================================================================================
SYNCHRONIZATION & REDUCTION OPERATIONS
================================================================================

1. Log-Likelihood Reduction:
   - Sum of siteLogLikelihood × weight across 648 patterns
   - Use parallel reduction tree or atomicAdd for doubles
   - Critical path operation

2. Expected Count Accumulation:
   - Vertex counts: atomicAdd for 4-element array per vertex
   - Edge counts: atomicAdd for 4x4 matrix per edge pair
   - Low collision rate with 648 patterns

================================================================================
RECOMMENDED IMPLEMENTATION PHASES
================================================================================

PHASE 1: Pattern-Level Parallelization (HIGHEST IMPACT)
---------------------------------------------------------
1. Parallelize ComputeLogLikelihoodUsingPatternsWithPropagation()
   - Launch 648 CUDA threads (or thread blocks)
   - Each thread: SetSite() → CalibrateTree() → ComputeLogLikelihood()
   - Parallel reduction for likelihood sum
   - Estimated speedup: 10-100x

2. Parallelize ComputeExpectedCounts() pattern loop
   - 648-pattern parallelization
   - Thread-local or atomic accumulation
   - Estimated speedup: 10-100x

PHASE 2: Message Passing Optimization (MEDIUM EFFORT)
-------------------------------------------------------
3. Level-synchronous CalibrateTree()
   - Process all cliques at same tree level in parallel
   - Synchronization barriers between levels
   - Parallelize belief computation
   - Estimated speedup: 2-5x

4. Optimize 4x4 matrix operations
   - Use shared memory for small matrices
   - Warp-level operations
   - Estimated speedup: 1.5-2x

PHASE 3: Advanced Optimizations (HIGH EFFORT)
-----------------------------------------------
5. GPU-resident message caching
6. Pipelined EM iterations
7. Asynchronous parameter updates

================================================================================
ESTIMATED PERFORMANCE GAINS
================================================================================

Component              | CPU Time | GPU Time | Speedup
-----------------------|----------|----------|--------
Pattern loop (648×)    | 100%     | 1-10%    | 10-100x
Message passing        | 100%     | 40-50%   | 2-3x
Belief computation     | 100%     | 10%      | 10x
Expected counts        | 100%     | 10%      | 10x
Total EM iteration     | 100%     | 8-15%    | 6-13x

Overall EM Algorithm:
- Current: 46 iterations × ~200ms = ~9 seconds
- With CUDA: 46 iterations × ~30ms = ~1.4 seconds
- Expected total speedup: 6-13x

================================================================================
KEY CUDA KERNEL SIGNATURES
================================================================================

// Kernel 1: Per-pattern log-likelihood
__global__ void cuda_compute_likelihood_per_pattern(
    int num_patterns,
    const PatternData* patterns,
    const CliqueTree* tree,
    const double* pattern_weights,
    double* output_likelihoods
);

// Kernel 2: Per-pattern expected counts
__global__ void cuda_compute_expected_counts_per_pattern(
    int pattern_idx,
    const PatternData* pattern,
    const CliqueTree* tree,
    double pattern_weight,
    double* expected_counts_vertex,    // atomic
    double* expected_counts_edge       // atomic
);

// Kernel 3: Parallel belief computation
__global__ void cuda_compute_beliefs(
    int num_cliques,
    const Clique* cliques,
    const Message* messages,
    Belief* output_beliefs
);

================================================================================
CONCLUSION
================================================================================

The EMBH phylogenetic inference code has EXTENSIVE parallelization opportunities:

1. Pattern-level computation (648 independent patterns) - HIGHEST IMPACT
   - Main computational bottleneck
   - Linear speedup potential: 10-100x

2. Expected counts accumulation (E-step) - HIGH IMPACT
   - Core EM algorithm step
   - Parallelizable with atomic operations

3. Belief computation in clique trees - MEDIUM IMPACT
   - Independent per clique after message passing

4. Tree message passing with level-synchronous approach - MEDIUM IMPACT
   - Limited by tree structure dependencies

The code's structure with packed pattern storage and modular components makes
it well-suited for GPU acceleration. Starting with Phase 1 (pattern-level
parallelization) alone could yield 10-100x speedup on the most computationally
intensive operations, reducing total EM algorithm runtime from ~9 seconds to
~1-2 seconds.
